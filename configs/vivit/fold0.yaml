# Video Transformer Ablation Configuration
# Video Vision Transformer (ViViT) for video understanding

# ==========================================
# EXPERIMENT CONFIGURATION
# ==========================================
experiment_name: "ablation_video_transformer_tb_classifier_fold0"
experiment_dir: "./checkpoints/ablation_video_transformer_tb_classifier_fold0"
seed: 42
device: "cuda"  # "cuda" or "cpu"

# Training control
train: true
evaluate_best_valid_model: true

# Model weights and resuming
model_weights: null  # Path to pretrained weights, null if none
best_model_path: null  # Path to best model for evaluation only
resume_from_checkpoint: null
reset_optimizers: false  # Whether to reset optimizer states when loading

# ==========================================
# ABLATION MODEL CONFIGURATION
# ==========================================

# Ablation model type - Video Transformer (ViViT)
model_type: "video_transformer"

# Active tasks - choose from: ['TB Label', 'Pneumonia Label', 'Covid Label']
active_tasks:
  - "TB Label"

# Task-specific weights for loss balancing
task_weights:
  "TB Label": 1.0

# Positive class weights for handling class imbalance
task_pos_weights:
  "TB Label": 1.4

# Pathology prediction configuration
use_pathology_loss: true
pathology_weight: 0.9
pathology_pos_weights: [1.0, 4.0, 15.0, 4.0]
num_pathologies: 4
pathology_classes:
  - "A-line"
  - "Large Consolidations"
  - "Pleural Effusion"
  - "Other Pathology"

# ==========================================
# DATA CONFIGURATION
# ==========================================

# Dataset paths - UPDATE THESE FOR YOUR DATA
root_dir: "/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/Data/CLUSSTER-Benin/cleaned_v3"
labels_csv: "/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/Data/CLUSSTER-Benin/cleaned_v2/labels/labels_multidiagnosis.csv"
file_metadata_csv: "/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/Data/CLUSSTER-Benin/cleaned_v3/processed_files_2.csv"
split_csv: "/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/Data/CLUSSTER-Benin/test_files/Fold_0.csv"
image_folder: "images"
video_folder: "videos"

# Data loading parameters
batch_size: 1  # Reduced for Video Transformer memory requirements
num_workers: 4  # Reduced for Video Transformer
frame_sampling: 16  # ViViT typically uses 16 frames
depth_filter: "15"  # Options: 'all', '5', '15'

# Image processing
target_height: 224  # Standard for ViViT
target_width: 224   # Standard for ViViT
in_channels: 3

# Site selection and ordering
files_per_site: "all"  # Can be integer or "all"
site_order: null  # Optional list of site names, null for default order
pad_missing_sites: true
max_sites: null  # Optional maximum number of sites
num_sites: 21
mode: "video"  # 'video', 'image', or 'both'
pooling: "attention"  # 'max', 'avg', or 'attention'

# ==========================================
# MODEL ARCHITECTURE
# ==========================================

# Model configuration
model_name: "ablation_video_transformer_tb_classifier_fold0"

# Model dimensions
hidden_dim: 512
dropout_rate: 0.3
num_classes: 1  # Binary classification for each task
classification_type: "binary"

# Video Transformer backbone configuration
backbone: "video_transformer"  # Using ViViT
freeze_backbone: false
pretrained: false  # ViViT not pretrained in this implementation

# ==========================================
# TRAINING CONFIGURATION
# ==========================================

# Training schedule
num_epochs: 30  # May need more epochs for Video Transformer
accumulation_steps: 16  # Larger accumulation due to smaller batch size
use_amp: true

# Learning rates and weight decay - adjusted for Video Transformer
learning_rate: 0.0005  # Higher LR for Video Transformer
weight_decay: 0.01  # Higher weight decay for transformer

# Backbone parameters - Video Transformer specific
backbone_lr: 0.0005  # Higher for Video Transformer
backbone_weight_decay: 0.01  # Higher weight decay
backbone_eta_min: 0.00001

# Pathology module parameters
pathology_lr: 0.001  # Higher for pathology modules
pathology_weight_decay: 0.001
pathology_eta_min: 0.00001

# Patient pipeline parameters
patient_pipeline_lr: 0.001  # Higher for patient pipeline
patient_pipeline_weight_decay: 0.001
patient_pipeline_eta_min: 0.00001

# Loss configuration
pos_weight: 1.4  # Default positive weight for binary classification

# ==========================================
# EVALUATION CONFIGURATION
# ==========================================

# Evaluation metrics and early stopping
eval_metric: "auc"  # Primary metric for model selection
eval_metric_goal: "max"  # 'max' or 'min'
early_stopping_patience: 12  # Longer patience for Video Transformer

# ==========================================
# DIRECTORIES
# ==========================================
log_dir: "logs"
save_dir: "models"
checkpoint_dir: "checkpoints"
pred_save_dir: "predictions"

# ==========================================
# TASK SPECIFIC CONFIGURATION
# ==========================================
task: "TB Label"