{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 07:49:03.730674: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756295343.750358 2083649 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756295343.756079 2083649 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Multi-Task Model Comprehensive Evaluation\n",
      "============================================================\n",
      "Found 1 folds to process\n",
      "Output directory: /gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/results_Aug11v1\n",
      "\n",
      "Fold 0:\n",
      "  Config: âœ“ /gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/configs/Finalruns/tb_drl_mil_Final_fold0.yaml\n",
      "  Model:  âœ“ /gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/checkpoints/drl_mil_tb_classifier_Aug26_fold0/checkpoint_best_metric_0.9174.pth\n",
      "\n",
      "âœ“ Found 1 valid folds: [0]\n",
      "\n",
      "============================================================\n",
      "ðŸ”„ Processing Fold 0\n",
      "============================================================\n",
      "Starting evaluation for fold 0...\n",
      "=== Loading Multi-Task Model and Data ===\n",
      "âœ“ Config loaded successfully\n",
      "  Active tasks: ['TB Label']\n",
      "  Selection strategy: RL\n",
      "  Use pathology loss: True\n",
      "  Experiment dir: ./checkpoints/drl_mil_tb_classifier_Aug26_fold0\n",
      "Active tasks: ['TB Label']\n",
      "Use pathology loss: True\n",
      "Loading checkpoint from: /gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/checkpoints/drl_mil_tb_classifier_Aug26_fold0/checkpoint_best_metric_0.9174.pth\n",
      "Checkpoint keys: ['epoch', 'model_state_dict', 'backbone_optimizer_state_dict', 'patient_pipeline_optimizer_state_dict', 'pathology_optimizers_state_dicts', 'schedulers_state_dicts', 'config', 'metrics', 'best_metric', 'best_epoch', 'epochs_without_improvement', 'active_tasks', 'use_pathology_loss', 'rl_optimizer_state_dict', 'frame_selector_temperature', 'backbone_scaler_state_dict', 'patient_pipeline_scaler_state_dict', 'pathology_scalers_state_dicts', 'frame_selector_scaler_state_dict']\n",
      "âœ“ Loaded model_state_dict from checkpoint\n",
      "  Checkpoint epoch: 4\n",
      "  Best metric: 0.9174\n",
      "  Checkpoint active tasks: ['TB Label']\n",
      "âœ“ Model moved to cuda:0 and set to eval mode\n",
      "Created dataset with 319 patients for train split\n",
      "Padding missing sites: True\n",
      "Created dataset with 81 patients for val split\n",
      "Padding missing sites: True\n",
      "Created dataset with 100 patients for test split\n",
      "Padding missing sites: True\n",
      "Running Test Data\n",
      "=== Starting Comprehensive Multi-Task Model Evaluation ===\n",
      "Processing 50 batches...\n",
      "Active tasks: ['TB Label']\n",
      "Use pathology loss: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [07:09<00:00,  8.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Statistics ===\n",
      "total_patients: 100\n",
      "total_sites: 2945\n",
      "total_batches: 50\n",
      "failed_batches: 0\n",
      "\n",
      "Creating DataFrames from 100 patient records and 2945 site records...\n",
      "Patient DataFrame shape: (100, 7)\n",
      "Site DataFrame shape: (2945, 26)\n",
      "Patient DataFrame columns: ['batch_idx', 'patient_id', 'num_valid_sites', 'tb_label_label', 'tb_label_logit', 'tb_label_prob', 'tb_label_pred']\n",
      "Site DataFrame columns: ['batch_idx', 'patient_id', 'site_position', 'site_index', 'site_mask', 'tb_label_label', 'tb_label_logit', 'tb_label_prob', 'tb_label_pred', 'a_lines_finding', 'b_lines_finding', 'small_consolidations_finding', 'large_consolidations_finding', 'a_lines_logit', 'a_lines_prob', 'a_lines_pred', 'b_lines_logit', 'b_lines_prob', 'b_lines_pred', 'small_consolidations_logit', 'small_consolidations_prob', 'small_consolidations_pred', 'large_consolidations_logit', 'large_consolidations_prob', 'large_consolidations_pred', 'mil_attention']\n",
      "\n",
      "TB Label targets distribution: [62 38]\n",
      "TB Label predictions distribution: [49 51]\n",
      "TB Label metrics calculated: ['accuracy', 'precision', 'recall', 'f1', 'specificity', 'auc', 'auprc']\n",
      "\n",
      "Complex data summary:\n",
      "  patient_features: 0 items\n",
      "  mil_attention: 100 items\n",
      "  site_features: 1270 items\n",
      "  site_rl_data: 1270 items\n",
      "  task_logits: 1 items\n",
      "  pathology_scores: 100 items\n",
      "=== Saving test Results ===\n",
      "=== Saving Comprehensive Results to /gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/results_Aug26 ===\n",
      "Filename base: test_multi_task_model_fold4\n",
      "âœ“ Patient data saved: /gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/results_Aug26/test_multi_task_model_fold4_patients.csv\n",
      "âœ“ Site data saved: /gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/results_Aug26/test_multi_task_model_fold4_sites.csv\n",
      "    MIL attention: 100 patients\n",
      "    Site features: 1270 sites\n",
      "    Task logits: 1 tasks\n",
      "    Pathology scores: 100 patients\n",
      "âœ“ Complex data saved: /gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/results_Aug26/test_multi_task_model_fold4_complex_data.h5\n",
      "âœ“ Metrics saved: /gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/results_Aug26/test_multi_task_model_fold4_metrics.json\n",
      "\n",
      "ðŸŽ‰ All results saved with base name: test_multi_task_model_fold4\n",
      "Running Val Data\n",
      "=== Starting Comprehensive Multi-Task Model Evaluation ===\n",
      "Processing 41 batches...\n",
      "Active tasks: ['TB Label']\n",
      "Use pathology loss: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   5%|â–         | 2/41 [00:44<12:01, 18.49s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import pickle\n",
    "import h5py\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "# Add both project root and src to path\n",
    "PROJECT_ROOT = \"/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP\"\n",
    "SRC_PATH = \"/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/src\"\n",
    "NETWORK_PATH = \"/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO\"\n",
    "\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "sys.path.insert(0, SRC_PATH)\n",
    "sys.path.insert(0, NETWORK_PATH)\n",
    "\n",
    "# Updated imports to match your new training system\n",
    "from Dataloaders.dataset_multitask import LungUltrasoundDataModule\n",
    "#from NetworkArchitecture.CLIP_Multitask_Aug5 import MultiTaskModel\n",
    "from NetworkArchitecture.CLIP_DRL_Aug26 import MultiTaskModel\n",
    "from config import load_config, MultiTaskConfig\n",
    "\n",
    "\n",
    "def evaluate_model_comprehensive(model, dataloader, device, active_tasks, use_pathology_loss=True, save_complex_data=True):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation for multi-task model that saves all model outputs and creates structured dataframes.\n",
    "    \n",
    "    Args:\n",
    "        model: MultiTaskModel instance\n",
    "        dataloader: DataLoader for evaluation\n",
    "        device: torch.device\n",
    "        active_tasks: List of active task names (e.g., ['TB Label', 'Pneumonia Label'])\n",
    "        use_pathology_loss: Whether pathology prediction is enabled\n",
    "        save_complex_data: Whether to save complex tensor data\n",
    "        \n",
    "    Returns:\n",
    "        - patient_df: Patient-level dataframe\n",
    "        - site_df: Site-level dataframe  \n",
    "        - complex_data: Dictionary with complex tensor data\n",
    "        - metrics: Evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"=== Starting Comprehensive Multi-Task Model Evaluation ===\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Storage for structured data\n",
    "    patient_records = []\n",
    "    site_records = []\n",
    "    \n",
    "    # Storage for complex data\n",
    "    complex_data = {\n",
    "        'patient_features': {},        # patient_id -> numpy array [feature_dim]\n",
    "        'mil_attention': {},          # patient_id -> numpy array [max_sites]\n",
    "        'site_features': {},          # (patient_id, site_idx) -> numpy array [feature_dim]\n",
    "        'site_rl_data': {},          # (patient_id, site_idx) -> dict\n",
    "        'task_logits': {},           # task_name -> patient_id -> numpy array\n",
    "    }\n",
    "    \n",
    "    if use_pathology_loss:\n",
    "        complex_data['pathology_scores'] = {}  # patient_id -> numpy array [max_sites, num_pathologies]\n",
    "    \n",
    "    # Statistics tracking\n",
    "    stats = {\n",
    "        'total_patients': 0,\n",
    "        'total_sites': 0,\n",
    "        'total_batches': 0,\n",
    "        'failed_batches': 0,\n",
    "    }\n",
    "    \n",
    "    pathology_names = ['a_lines', 'b_lines', 'small_consolidations', 'large_consolidations', 'pleural_effusion']\n",
    "    \n",
    "    print(f\"Processing {len(dataloader)} batches...\")\n",
    "    print(f\"Active tasks: {active_tasks}\")\n",
    "    print(f\"Use pathology loss: {use_pathology_loss}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "            try:\n",
    "                # Extract batch data\n",
    "                patient_ids = batch['patient_ids']\n",
    "                site_videos = batch['site_videos'].to(device)\n",
    "                site_indices = batch['site_indices'].to(device)\n",
    "                site_masks = batch['site_masks'].to(device)\n",
    "                site_findings = batch['site_findings'].to(device)\n",
    "                \n",
    "                # Multi-task labels\n",
    "                tb_labels = batch['tb_labels'].to(device).float()\n",
    "                pneumonia_labels = batch['pneumonia_labels'].to(device).float()\n",
    "                covid_labels = batch['covid_labels'].to(device).float()\n",
    "                \n",
    "                batch_size = len(patient_ids)\n",
    "                stats['total_patients'] += batch_size\n",
    "                stats['total_batches'] += 1\n",
    "                \n",
    "                # Prepare model inputs\n",
    "                inputs = {\n",
    "                    'site_videos': site_videos,\n",
    "                    'site_indices': site_indices,\n",
    "                    'site_masks': site_masks,\n",
    "                    'site_findings': site_findings,\n",
    "                    'is_patient_level': True\n",
    "                }\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Extract model outputs\n",
    "                task_logits = outputs.get('task_logits', {})\n",
    "                \n",
    "                # Get multi-task probabilities and predictions\n",
    "                task_probs = {}\n",
    "                task_preds = {}\n",
    "                for task_name in active_tasks:\n",
    "                    if task_name in task_logits:\n",
    "                        task_probs[task_name] = torch.sigmoid(task_logits[task_name])\n",
    "                        task_preds[task_name] = (task_probs[task_name] > 0.5).float()\n",
    "                \n",
    "                # Extract other outputs\n",
    "                mil_attention = outputs.get('mil_attention', None)\n",
    "                patient_features = outputs.get('patient_features', None)\n",
    "                site_features = outputs.get('site_features', None)\n",
    "                site_rl_data = outputs.get('site_rl_data', None)\n",
    "                pathology_scores = outputs.get('pathology_scores', None) if use_pathology_loss else None\n",
    "                \n",
    "                # Process each patient in the batch\n",
    "                for i in range(batch_size):\n",
    "                    patient_id = patient_ids[i]\n",
    "                    \n",
    "                    # Get task labels and predictions for this patient\n",
    "                    task_labels = {}\n",
    "                    task_logits_patient = {}\n",
    "                    task_probs_patient = {}\n",
    "                    task_preds_patient = {}\n",
    "                    \n",
    "                    if 'TB Label' in active_tasks:\n",
    "                        task_labels['TB Label'] = tb_labels[i].cpu().item()\n",
    "                        if 'TB Label' in task_logits:\n",
    "                            task_logits_patient['TB Label'] = task_logits['TB Label'][i].cpu().item()\n",
    "                            task_probs_patient['TB Label'] = task_probs['TB Label'][i].cpu().item()\n",
    "                            task_preds_patient['TB Label'] = task_preds['TB Label'][i].cpu().item()\n",
    "                    \n",
    "                    if 'Pneumonia Label' in active_tasks:\n",
    "                        task_labels['Pneumonia Label'] = pneumonia_labels[i].cpu().item()\n",
    "                        if 'Pneumonia Label' in task_logits:\n",
    "                            task_logits_patient['Pneumonia Label'] = task_logits['Pneumonia Label'][i].cpu().item()\n",
    "                            task_probs_patient['Pneumonia Label'] = task_probs['Pneumonia Label'][i].cpu().item()\n",
    "                            task_preds_patient['Pneumonia Label'] = task_preds['Pneumonia Label'][i].cpu().item()\n",
    "                    \n",
    "                    if 'Covid Label' in active_tasks:\n",
    "                        task_labels['Covid Label'] = covid_labels[i].cpu().item()\n",
    "                        if 'Covid Label' in task_logits:\n",
    "                            task_logits_patient['Covid Label'] = task_logits['Covid Label'][i].cpu().item()\n",
    "                            task_probs_patient['Covid Label'] = task_probs['Covid Label'][i].cpu().item()\n",
    "                            task_preds_patient['Covid Label'] = task_preds['Covid Label'][i].cpu().item()\n",
    "                    \n",
    "                    # Get number of valid sites for this patient\n",
    "                    num_sites = site_masks[i].sum().item()\n",
    "                    stats['total_sites'] += num_sites\n",
    "                    \n",
    "                    # Store patient-level complex data\n",
    "                    if save_complex_data:\n",
    "                        if patient_features is not None:\n",
    "                            complex_data['patient_features'][patient_id] = patient_features[i].cpu().numpy()\n",
    "                        \n",
    "                        if mil_attention is not None:\n",
    "                            complex_data['mil_attention'][patient_id] = mil_attention[i].cpu().numpy()\n",
    "                        \n",
    "                        # Store task logits\n",
    "                        for task_name in active_tasks:\n",
    "                            if task_name not in complex_data['task_logits']:\n",
    "                                complex_data['task_logits'][task_name] = {}\n",
    "                            if task_name in task_logits:\n",
    "                                complex_data['task_logits'][task_name][patient_id] = task_logits[task_name][i].cpu().numpy()\n",
    "                        \n",
    "                        if use_pathology_loss and pathology_scores is not None:\n",
    "                            complex_data['pathology_scores'][patient_id] = pathology_scores[i].cpu().numpy()\n",
    "                    \n",
    "                    # Create patient-level record\n",
    "                    patient_record = {\n",
    "                        'batch_idx': batch_idx,\n",
    "                        'patient_id': patient_id,\n",
    "                        'num_valid_sites': num_sites,\n",
    "                    }\n",
    "                    \n",
    "                    # Add task-specific fields\n",
    "                    for task_name in active_tasks:\n",
    "                        prefix = task_name.lower().replace(' ', '_')\n",
    "                        patient_record[f'{prefix}_label'] = task_labels.get(task_name, -1)\n",
    "                        patient_record[f'{prefix}_logit'] = task_logits_patient.get(task_name, float('nan'))\n",
    "                        patient_record[f'{prefix}_prob'] = task_probs_patient.get(task_name, float('nan'))\n",
    "                        patient_record[f'{prefix}_pred'] = task_preds_patient.get(task_name, float('nan'))\n",
    "                    \n",
    "                    patient_records.append(patient_record)\n",
    "                    \n",
    "                    # Process each valid site for this patient\n",
    "                    for s in range(num_sites):\n",
    "                        site_idx = site_indices[i, s].cpu().item()\n",
    "                        site_finding = site_findings[i, s].cpu().numpy()\n",
    "                        site_mask_value = site_masks[i, s].cpu().item()\n",
    "                        \n",
    "                        # Create site-level record\n",
    "                        site_record = {\n",
    "                            'batch_idx': batch_idx,\n",
    "                            'patient_id': patient_id,\n",
    "                            'site_position': s,  # Position within patient's sites (0, 1, 2, ...)\n",
    "                            'site_index': site_idx,  # Anatomical site index\n",
    "                            'site_mask': site_mask_value,\n",
    "                        }\n",
    "                        \n",
    "                        # Add task labels and predictions for reference\n",
    "                        for task_name in active_tasks:\n",
    "                            prefix = task_name.lower().replace(' ', '_')\n",
    "                            site_record[f'{prefix}_label'] = task_labels.get(task_name, -1)\n",
    "                            site_record[f'{prefix}_logit'] = task_logits_patient.get(task_name, float('nan'))\n",
    "                            site_record[f'{prefix}_prob'] = task_probs_patient.get(task_name, float('nan'))\n",
    "                            site_record[f'{prefix}_pred'] = task_preds_patient.get(task_name, float('nan'))\n",
    "                        \n",
    "                        # Add site findings (ground truth pathology labels)\n",
    "                        for p_idx, p_name in enumerate(pathology_names):\n",
    "                            if p_idx < len(site_finding):\n",
    "                                site_record[f'{p_name}_finding'] = site_finding[p_idx]\n",
    "                        \n",
    "                        # Add site pathology predictions if available\n",
    "                        if use_pathology_loss and pathology_scores is not None:\n",
    "                            site_path_scores = pathology_scores[i, s].cpu().numpy()\n",
    "                            \n",
    "                            for p_idx, p_name in enumerate(pathology_names):\n",
    "                                if p_idx < len(site_path_scores):\n",
    "                                    site_record[f'{p_name}_logit'] = site_path_scores[p_idx]\n",
    "                                    site_record[f'{p_name}_prob'] = 1 / (1 + np.exp(-site_path_scores[p_idx]))\n",
    "                                    site_record[f'{p_name}_pred'] = int(site_path_scores[p_idx] > 0)\n",
    "                        \n",
    "                        # Add MIL attention for this site if available\n",
    "                        if mil_attention is not None:\n",
    "                            site_record['mil_attention'] = mil_attention[i, s].cpu().item()\n",
    "                        \n",
    "                        # Store site-level complex data\n",
    "                        if save_complex_data:\n",
    "                            if site_features is not None:\n",
    "                                complex_data['site_features'][(patient_id, site_idx)] = site_features[i, s].cpu().numpy()\n",
    "                            \n",
    "                            if site_rl_data is not None and i < len(site_rl_data) and s < len(site_rl_data[i]):\n",
    "                                # Convert RL data to serializable format\n",
    "                                rl_data_item = site_rl_data[i][s]\n",
    "                                serializable_rl_data = {}\n",
    "                                \n",
    "                                for key, value in rl_data_item.items():\n",
    "                                    if isinstance(value, torch.Tensor):\n",
    "                                        serializable_rl_data[key] = value.cpu().numpy()\n",
    "                                    else:\n",
    "                                        serializable_rl_data[key] = value\n",
    "                                \n",
    "                                complex_data['site_rl_data'][(patient_id, site_idx)] = serializable_rl_data\n",
    "                        \n",
    "                        site_records.append(site_record)\n",
    "                \n",
    "                # Memory management\n",
    "                if batch_idx % 50 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if 'out of memory' in str(e).lower():\n",
    "                    print(f\"âŒ OOM during evaluation at batch {batch_idx}, skipping\")\n",
    "                    stats['failed_batches'] += 1\n",
    "                    torch.cuda.empty_cache()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error at batch {batch_idx}: {e}\")\n",
    "                stats['failed_batches'] += 1\n",
    "                continue\n",
    "    \n",
    "    print(\"\\n=== Evaluation Statistics ===\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Create DataFrames\n",
    "    print(f\"\\nCreating DataFrames from {len(patient_records)} patient records and {len(site_records)} site records...\")\n",
    "    \n",
    "    patient_df = pd.DataFrame(patient_records)\n",
    "    site_df = pd.DataFrame(site_records)\n",
    "    \n",
    "    print(f\"Patient DataFrame shape: {patient_df.shape}\")\n",
    "    print(f\"Site DataFrame shape: {site_df.shape}\")\n",
    "    print(f\"Patient DataFrame columns: {list(patient_df.columns)}\")\n",
    "    print(f\"Site DataFrame columns: {list(site_df.columns)}\")\n",
    "    \n",
    "    # Calculate metrics for each active task\n",
    "    metrics = {}\n",
    "    \n",
    "    for task_name in active_tasks:\n",
    "        if len(patient_df) > 0:\n",
    "            prefix = task_name.lower().replace(' ', '_')\n",
    "            label_col = f'{prefix}_label'\n",
    "            prob_col = f'{prefix}_prob'\n",
    "            pred_col = f'{prefix}_pred'\n",
    "            \n",
    "            if label_col in patient_df.columns and prob_col in patient_df.columns:\n",
    "                # Filter out invalid labels\n",
    "                valid_mask = patient_df[label_col] >= 0\n",
    "                \n",
    "                if valid_mask.sum() > 0:\n",
    "                    patient_targets = patient_df.loc[valid_mask, label_col].values\n",
    "                    patient_probs = patient_df.loc[valid_mask, prob_col].values\n",
    "                    patient_preds = patient_df.loc[valid_mask, pred_col].values\n",
    "                    \n",
    "                    print(f\"\\n{task_name} targets distribution: {np.bincount(patient_targets.astype(int))}\")\n",
    "                    print(f\"{task_name} predictions distribution: {np.bincount(patient_preds.astype(int))}\")\n",
    "                    \n",
    "                    try:\n",
    "                        task_metrics = calculate_metrics(patient_targets, patient_preds, probabilities=patient_probs)\n",
    "                        # Add task prefix to metrics\n",
    "                        for key, value in task_metrics.items():\n",
    "                            metrics[f'{task_name}_{key}'] = value\n",
    "                        print(f\"{task_name} metrics calculated: {list(task_metrics.keys())}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not calculate metrics for {task_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nComplex data summary:\")\n",
    "    for key, data_dict in complex_data.items():\n",
    "        if isinstance(data_dict, dict):\n",
    "            print(f\"  {key}: {len(data_dict)} items\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(data_dict)}\")\n",
    "    \n",
    "    return patient_df, site_df, complex_data, metrics\n",
    "\n",
    "\n",
    "def calculate_metrics(targets, predictions, probabilities=None):\n",
    "    \"\"\"Calculate standard classification metrics.\"\"\"\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, precision_score, recall_score, f1_score,\n",
    "        roc_auc_score, average_precision_score, confusion_matrix\n",
    "    )\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    try:\n",
    "        metrics['accuracy'] = accuracy_score(targets, predictions)\n",
    "        metrics['precision'] = precision_score(targets, predictions, zero_division=0)\n",
    "        metrics['recall'] = recall_score(targets, predictions, zero_division=0)\n",
    "        metrics['f1'] = f1_score(targets, predictions, zero_division=0)\n",
    "        \n",
    "        # Calculate specificity manually\n",
    "        tn, fp, fn, tp = confusion_matrix(targets, predictions).ravel()\n",
    "        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        \n",
    "        if probabilities is not None and len(np.unique(targets)) > 1:\n",
    "            metrics['auc'] = roc_auc_score(targets, probabilities)\n",
    "            metrics['auprc'] = average_precision_score(targets, probabilities)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics: {e}\")\n",
    "        # Provide default values\n",
    "        metrics['accuracy'] = 0.0\n",
    "        metrics['auc'] = 0.5\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_comprehensive_results(patient_df, site_df, complex_data, metrics, output_dir, split_name, experiment_name, fold_num):\n",
    "    \"\"\"Save comprehensive evaluation results in multiple formats.\"\"\"\n",
    "    \n",
    "    print(f\"=== Saving Comprehensive Results to {output_dir} ===\")\n",
    "    \n",
    "    # Create filename base\n",
    "    if fold_num is not None:\n",
    "        filename_base = f'{split_name}_{experiment_name}_fold{fold_num}'\n",
    "    else:\n",
    "        filename_base = f'{split_name}_{experiment_name}'\n",
    "    \n",
    "    print(f\"Filename base: {filename_base}\")\n",
    "    \n",
    "    saved_files = {}\n",
    "    \n",
    "    # 1. Save patient-level dataframe\n",
    "    patient_csv_path = os.path.join(output_dir, f'{filename_base}_patients.csv')\n",
    "    patient_df.to_csv(patient_csv_path, index=False)\n",
    "    saved_files['patient_csv'] = patient_csv_path\n",
    "    print(f\"âœ“ Patient data saved: {patient_csv_path}\")\n",
    "    \n",
    "    # 2. Save site-level dataframe\n",
    "    site_csv_path = os.path.join(output_dir, f'{filename_base}_sites.csv')\n",
    "    site_df.to_csv(site_csv_path, index=False)\n",
    "    saved_files['site_csv'] = site_csv_path\n",
    "    print(f\"âœ“ Site data saved: {site_csv_path}\")\n",
    "    \n",
    "    # 3. Save complex data using HDF5\n",
    "    hdf5_path = os.path.join(output_dir, f'{filename_base}_complex_data.h5')\n",
    "    \n",
    "    try:\n",
    "        with h5py.File(hdf5_path, 'w') as f:\n",
    "            # Add metadata\n",
    "            f.attrs['split'] = split_name\n",
    "            f.attrs['experiment_name'] = experiment_name\n",
    "            if fold_num is not None:\n",
    "                f.attrs['fold'] = fold_num\n",
    "            f.attrs['num_patients'] = len(patient_df)\n",
    "            f.attrs['num_sites'] = len(site_df)\n",
    "            \n",
    "            # Patient features\n",
    "            if complex_data['patient_features']:\n",
    "                patient_grp = f.create_group('patient_features')\n",
    "                for patient_id, features in complex_data['patient_features'].items():\n",
    "                    patient_grp.create_dataset(str(patient_id), data=features)\n",
    "                print(f\"    Patient features: {len(complex_data['patient_features'])} patients\")\n",
    "            \n",
    "            # MIL attention\n",
    "            if complex_data['mil_attention']:\n",
    "                mil_grp = f.create_group('mil_attention')\n",
    "                for patient_id, attention in complex_data['mil_attention'].items():\n",
    "                    mil_grp.create_dataset(str(patient_id), data=attention)\n",
    "                print(f\"    MIL attention: {len(complex_data['mil_attention'])} patients\")\n",
    "            \n",
    "            # Site features\n",
    "            if complex_data['site_features']:\n",
    "                site_grp = f.create_group('site_features')\n",
    "                for (patient_id, site_idx), features in complex_data['site_features'].items():\n",
    "                    dataset_name = f\"{patient_id}_site_{site_idx}\"\n",
    "                    site_grp.create_dataset(dataset_name, data=features)\n",
    "                print(f\"    Site features: {len(complex_data['site_features'])} sites\")\n",
    "            \n",
    "            # Task logits\n",
    "            if complex_data['task_logits']:\n",
    "                task_grp = f.create_group('task_logits')\n",
    "                for task_name, task_data in complex_data['task_logits'].items():\n",
    "                    task_subgrp = task_grp.create_group(task_name.replace(' ', '_'))\n",
    "                    for patient_id, logits in task_data.items():\n",
    "                        task_subgrp.create_dataset(str(patient_id), data=logits)\n",
    "                print(f\"    Task logits: {len(complex_data['task_logits'])} tasks\")\n",
    "            \n",
    "            # Pathology scores\n",
    "            if 'pathology_scores' in complex_data and complex_data['pathology_scores']:\n",
    "                pathology_grp = f.create_group('pathology_scores')\n",
    "                for patient_id, scores in complex_data['pathology_scores'].items():\n",
    "                    pathology_grp.create_dataset(str(patient_id), data=scores)\n",
    "                print(f\"    Pathology scores: {len(complex_data['pathology_scores'])} patients\")\n",
    "        \n",
    "        saved_files['complex_hdf5'] = hdf5_path\n",
    "        print(f\"âœ“ Complex data saved: {hdf5_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  HDF5: Failed to save ({e})\")\n",
    "    \n",
    "    # 4. Save metrics as JSON\n",
    "    metrics_path = os.path.join(output_dir, f'{filename_base}_metrics.json')\n",
    "    try:\n",
    "        # Convert numpy types to Python types for JSON serialization\n",
    "        json_metrics = {}\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, (np.integer, np.floating)):\n",
    "                json_metrics[key] = value.item()\n",
    "            else:\n",
    "                json_metrics[key] = value\n",
    "        \n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(json_metrics, f, indent=2)\n",
    "        saved_files['metrics_json'] = metrics_path\n",
    "        print(f\"âœ“ Metrics saved: {metrics_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Metrics: Failed to save ({e})\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ All results saved with base name: {filename_base}\")\n",
    "    return saved_files\n",
    "\n",
    "\n",
    "def run_comprehensive_evaluation(test_config):\n",
    "    \"\"\"Run the comprehensive evaluation pipeline for multi-task model.\"\"\"\n",
    "    \n",
    "    print(\"=== Loading Multi-Task Model and Data ===\")\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(f'cuda:{test_config[\"gpu_id\"]}' if test_config['gpu_id'] >= 0 and torch.cuda.is_available() else 'cpu')\n",
    "    if test_config['gpu_id'] >= 0 and torch.cuda.is_available():\n",
    "        torch.cuda.set_device(test_config['gpu_id'])\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(test_config['output_dir'], exist_ok=True)\n",
    "\n",
    "    # Load configuration using the actual config system\n",
    "    config = load_config(config_file=test_config['config_path'])\n",
    "    \n",
    "    print(\"âœ“ Config loaded successfully\")\n",
    "    print(f\"  Active tasks: {getattr(config, 'active_tasks', 'Not specified')}\")\n",
    "    print(f\"  Selection strategy: {getattr(config, 'selection_strategy', 'Not specified')}\")\n",
    "    print(f\"  Use pathology loss: {getattr(config, 'use_pathology_loss', 'Not specified')}\")\n",
    "    print(f\"  Experiment dir: {getattr(config, 'experiment_dir', 'Not specified')}\")\n",
    "    \n",
    "    # Extract multi-task configuration\n",
    "    active_tasks = getattr(config, 'active_tasks', ['TB Label'])\n",
    "    use_pathology_loss = getattr(config, 'use_pathology_loss', True)\n",
    "    \n",
    "    print(f\"Active tasks: {active_tasks}\")\n",
    "    print(f\"Use pathology loss: {use_pathology_loss}\")\n",
    "    \n",
    "    # Setup data module\n",
    "    data_module = LungUltrasoundDataModule(\n",
    "        root_dir=config.root_dir,\n",
    "        labels_csv=config.labels_csv,\n",
    "        file_metadata_csv=config.file_metadata_csv,\n",
    "        image_folder=config.image_folder,\n",
    "        video_folder=config.video_folder,\n",
    "        split_csv=config.split_csv,\n",
    "        batch_size=config.batch_size,\n",
    "        num_workers=config.num_workers,\n",
    "        frame_sampling=config.frame_sampling,\n",
    "        depth_filter=config.depth_filter,\n",
    "        cache_size=100,\n",
    "        files_per_site=getattr(config, 'files_per_site', 'all'), \n",
    "        site_order=getattr(config, 'site_order', None),\n",
    "        pad_missing_sites=getattr(config, 'pad_missing_sites', True),    \n",
    "        max_sites=getattr(config, 'max_sites', None),\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = MultiTaskModel(config)\n",
    "\n",
    "    # Use model_path from config if available, otherwise use test_config\n",
    "    model_path = getattr(config, 'model_path', None) or test_config['model_path']\n",
    "    print(f\"Loading checkpoint from: {model_path}\")\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "    # Print checkpoint info\n",
    "    print(f\"Checkpoint keys: {list(checkpoint.keys())}\")\n",
    "\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"âœ“ Loaded model_state_dict from checkpoint\")\n",
    "        \n",
    "        # Print additional checkpoint info if available\n",
    "        if 'epoch' in checkpoint:\n",
    "            print(f\"  Checkpoint epoch: {checkpoint['epoch']}\")\n",
    "        if 'best_metric' in checkpoint:\n",
    "            print(f\"  Best metric: {checkpoint['best_metric']:.4f}\")\n",
    "        if 'active_tasks' in checkpoint:\n",
    "            print(f\"  Checkpoint active tasks: {checkpoint['active_tasks']}\")\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"âœ“ Loaded state dict directly from checkpoint\")\n",
    "        \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"âœ“ Model moved to {device} and set to eval mode\")\n",
    "\n",
    "    data_module.setup(stage='patient_level')\n",
    "\n",
    "    def run_split_evaluation(split_name, dataloader):\n",
    "        \"\"\"Helper function to run evaluation on a specific split.\"\"\"\n",
    "        patient_df, site_df, complex_data, metrics = evaluate_model_comprehensive(\n",
    "            model, dataloader, device, active_tasks, use_pathology_loss, test_config['save_complex_data']\n",
    "        )\n",
    "        \n",
    "        print(f\"=== Saving {split_name} Results ===\")\n",
    "        \n",
    "        # Save results\n",
    "        saved_files = save_comprehensive_results(\n",
    "            patient_df, site_df, complex_data, metrics, \n",
    "            test_config['output_dir'], split_name, 'multi_task_model', test_config['fold']\n",
    "        )\n",
    "        \n",
    "        return patient_df, site_df, complex_data, metrics, saved_files\n",
    "\n",
    "    # Run evaluation based on split configuration\n",
    "    if test_config['split'] == 'train':\n",
    "        train_dataloader = data_module.patient_level_dataloader('train')\n",
    "        return run_split_evaluation('train', train_dataloader)\n",
    "\n",
    "    elif test_config['split'] == 'val':\n",
    "        val_dataloader = data_module.patient_level_dataloader('val')\n",
    "        return run_split_evaluation('val', val_dataloader)\n",
    "\n",
    "    elif test_config['split'] == 'test':\n",
    "        test_dataloader = data_module.patient_level_dataloader('test')\n",
    "        return run_split_evaluation('test', test_dataloader)\n",
    "\n",
    "    else:  # 'all' or any other value\n",
    "        train_dataloader = data_module.patient_level_dataloader('train')\n",
    "        val_dataloader = data_module.patient_level_dataloader('val')\n",
    "        test_dataloader = data_module.patient_level_dataloader('test')\n",
    "\n",
    "        # Save Train Data\n",
    "\n",
    "        \n",
    "        # Save Test Data\n",
    "        print(\"Running Test Data\")\n",
    "        test_results = run_split_evaluation('test', test_dataloader)\n",
    "        \n",
    "        # Save Val Data\n",
    "        print(\"Running Val Data\")\n",
    "        val_results = run_split_evaluation('val', val_dataloader)\n",
    "\n",
    "        print(\"Running Train Data\")\n",
    "        train_results = run_split_evaluation('train', train_dataloader)\n",
    "        \n",
    "        return test_results  # Return test results as primary\n",
    "\n",
    "\n",
    "def analyze_saved_results(patient_df, site_df, active_tasks, use_pathology_loss=True):\n",
    "    \"\"\"Analyze the saved results and generate insights.\"\"\"\n",
    "    \n",
    "    print(\"=== Multi-Task Data Analysis ===\")\n",
    "    \n",
    "    # Patient-level analysis for each task\n",
    "    print(f\"\\nPatient-level Analysis:\")\n",
    "    print(f\"  Total patients: {len(patient_df)}\")\n",
    "    \n",
    "    for task_name in active_tasks:\n",
    "        prefix = task_name.lower().replace(' ', '_')\n",
    "        label_col = f'{prefix}_label'\n",
    "        prob_col = f'{prefix}_prob'\n",
    "        \n",
    "        if label_col in patient_df.columns:\n",
    "            valid_mask = patient_df[label_col] >= 0\n",
    "            if valid_mask.sum() > 0:\n",
    "                positive_count = patient_df.loc[valid_mask, label_col].sum()\n",
    "                total_count = valid_mask.sum()\n",
    "                avg_prob = patient_df.loc[valid_mask, prob_col].mean() if prob_col in patient_df.columns else 0\n",
    "                print(f\"  {task_name}: {positive_count}/{total_count} ({positive_count/total_count*100:.1f}%)\")\n",
    "                print(f\"    Average probability: {avg_prob:.3f}\")\n",
    "    \n",
    "    # Site-level analysis\n",
    "    print(f\"\\nSite-level Analysis:\")\n",
    "    print(f\"  Total sites: {len(site_df)}\")\n",
    "    print(f\"  Sites per patient: {len(site_df) / len(patient_df):.1f}\")\n",
    "    print(f\"  Unique anatomical sites: {site_df['site_index'].nunique()}\")\n",
    "    print(f\"  Site index range: {site_df['site_index'].min()}-{site_df['site_index'].max()}\")\n",
    "    \n",
    "    # Pathology analysis\n",
    "    if use_pathology_loss:\n",
    "        pathology_cols = [col for col in site_df.columns if col.endswith('_finding')]\n",
    "        print(f\"\\nPathology Findings (Ground Truth):\")\n",
    "        for col in pathology_cols:\n",
    "            pathology_name = col.replace('_finding', '').replace('_', ' ').title()\n",
    "            positive_sites = site_df[col].sum()\n",
    "            total_sites = len(site_df)\n",
    "            print(f\"  {pathology_name}: {positive_sites}/{total_sites} ({positive_sites/total_sites*100:.1f}%)\")\n",
    "        \n",
    "        # Prediction analysis\n",
    "        pathology_pred_cols = [col for col in site_df.columns if col.endswith('_pred')]\n",
    "        if pathology_pred_cols:\n",
    "            print(f\"\\nPathology Predictions:\")\n",
    "            for col in pathology_pred_cols:\n",
    "                pathology_name = col.replace('_pred', '').replace('_', ' ').title()\n",
    "                predicted_positive = site_df[col].sum()\n",
    "                total_sites = len(site_df)\n",
    "                print(f\"  {pathology_name}: {predicted_positive}/{total_sites} ({predicted_positive/total_sites*100:.1f}%)\")\n",
    "    \n",
    "    return patient_df, site_df\n",
    "\n",
    "\n",
    "def create_cross_fold_summary(successful_folds, results_dir):\n",
    "    \"\"\"Create a summary across all successful folds.\"\"\"\n",
    "    print(\"Creating cross-fold summary...\")\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    for fold in successful_folds:\n",
    "        # Try to load metrics from each fold\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            metrics_file = os.path.join(results_dir, f'{split}_multi_task_model_fold{fold}_metrics.json')\n",
    "            if os.path.exists(metrics_file):\n",
    "                try:\n",
    "                    with open(metrics_file, 'r') as f:\n",
    "                        fold_metrics = json.load(f)\n",
    "                    \n",
    "                    # Add fold and split information\n",
    "                    fold_metrics['fold'] = fold\n",
    "                    fold_metrics['split'] = split\n",
    "                    all_metrics.append(fold_metrics)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not load metrics from {metrics_file}: {e}\")\n",
    "    \n",
    "    if all_metrics:\n",
    "        # Convert to DataFrame and save\n",
    "        summary_df = pd.DataFrame(all_metrics)\n",
    "        summary_file = os.path.join(results_dir, 'cross_fold_summary.csv')\n",
    "        summary_df.to_csv(summary_file, index=False)\n",
    "        \n",
    "        print(f\"âœ“ Cross-fold summary saved to: {summary_file}\")\n",
    "        \n",
    "        # Print average metrics\n",
    "        print(\"\\nðŸ“Š Average Metrics Across Folds:\")\n",
    "        numeric_cols = summary_df.select_dtypes(include=[np.number]).columns\n",
    "        avg_metrics = summary_df.groupby('split')[numeric_cols].mean()\n",
    "        \n",
    "        for split in ['train', 'val', 'test']:\n",
    "            if split in avg_metrics.index:\n",
    "                print(f\"\\n{split.upper()} (avg across {len(successful_folds)} folds):\")\n",
    "                for col in ['TB Label_auc', 'TB Label_accuracy', 'Pneumonia Label_auc', 'Pneumonia Label_accuracy']:\n",
    "                    if col in avg_metrics.columns:\n",
    "                        print(f\"  {col}: {avg_metrics.loc[split, col]:.4f}\")\n",
    "    else:\n",
    "        print(\"No metrics files found for cross-fold summary\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸš€ Starting Multi-Task Model Comprehensive Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Configuration paths for all folds - UPDATE THESE TO MATCH YOUR ACTUAL CONFIG FILES\n",
    "    config_paths = [\n",
    "       # '/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/config/experiments/multitask/multitask_fold0_config.yaml',\n",
    "       # '/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/config/experiments/multitask/multitask_fold1_config.yaml',\n",
    "       # '/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/config/experiments/multitask/multitask_fold2_config.yaml',\n",
    "        #'/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/config/experiments/multitask/multitask_fold3_config.yaml',\n",
    "       # '/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/config/experiments/multitask/multitask_fold4_config.yaml'\n",
    "       '/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/configs/Finalruns/tb_drl_mil_Final_fold0.yaml'\n",
    "    ]\n",
    "    \n",
    "    # Model paths can be None if they're specified in the config files\n",
    "    # model_paths = [\n",
    "    #     None,  # Will use model_path from config file\n",
    "    #     None,  # Will use model_path from config file\n",
    "    #     None,  # Will use model_path from config file\n",
    "    #     None,  # Will use model_path from config file\n",
    "    #     None,  # Will use model_path from config file\n",
    "    # ]\n",
    "    \n",
    "    # Alternative: If you want to override model paths explicitly, use:\n",
    "    model_paths = [\n",
    "     #   '/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/experiments/multitask_tb_pneumonia_attention_fold0/checkpoint_best_metric_0.9245.pth',\n",
    "      #  '/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/experiments/multitask_tb_pneumonia_attention_fold1/checkpoint_best.pth',\n",
    "       # '/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/experiments/multitask_tb_pneumonia_attention_fold2/checkpoint_best.pth',\n",
    "       # '/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/experiments/multitask_tb_pneumonia_attention_fold3/checkpoint_best.pth',\n",
    "       # '/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/experiments/multitask_tb_pneumonia_attention_fold4/checkpoint_best.pth',\n",
    "        '/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/checkpoints/drl_mil_tb_classifier_Aug26_fold0/checkpoint_best_metric_0.9174.pth'\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(config_paths)} folds to process\")\n",
    "    print(f\"Output directory: /gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/results_Aug11v1\")\n",
    "    \n",
    "    # First, check which files exist\n",
    "    valid_folds = []\n",
    "    for i in range(len(config_paths)):\n",
    "        config_exists = os.path.exists(config_paths[i])\n",
    "        \n",
    "        # If model_path is None, we'll check config file for model_path\n",
    "        if model_paths[i] is None:\n",
    "            model_exists = True  # Will be validated when loading config\n",
    "        else:\n",
    "            model_exists = os.path.exists(model_paths[i])\n",
    "        \n",
    "        print(f\"\\nFold {i}:\")\n",
    "        print(f\"  Config: {'âœ“' if config_exists else 'âŒ'} {config_paths[i]}\")\n",
    "        if model_paths[i] is not None:\n",
    "            print(f\"  Model:  {'âœ“' if model_exists else 'âŒ'} {model_paths[i]}\")\n",
    "        else:\n",
    "            print(f\"  Model:  Will use path from config file\")\n",
    "        \n",
    "        if config_exists and model_exists:\n",
    "            valid_folds.append(i)\n",
    "        else:\n",
    "            print(f\"  Status: âŒ Skipping fold {i} (missing files)\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Found {len(valid_folds)} valid folds: {valid_folds}\")\n",
    "    \n",
    "    if not valid_folds:\n",
    "        print(\"âŒ No valid folds found. Please check the file paths.\")\n",
    "        exit()\n",
    "    \n",
    "    # Process each valid fold\n",
    "    successful_folds = []\n",
    "    failed_folds = []\n",
    "    \n",
    "    for i in valid_folds:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ðŸ”„ Processing Fold {i}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        test_config = {\n",
    "            'config_path': config_paths[i],\n",
    "            'model_path': model_paths[i],  # Can be None if specified in config\n",
    "            'split': 'all',\n",
    "            'fold': 4,\n",
    "            'output_dir': '/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/results_Aug26',\n",
    "            'gpu_id': 0,\n",
    "            'save_complex_data': True,\n",
    "            'batch_size_override': None,\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            print(f\"Starting evaluation for fold {i}...\")\n",
    "            patient_df, site_df, complex_data, metrics, saved_files = run_comprehensive_evaluation(test_config)\n",
    "            \n",
    "            print(f\"âœ… Fold {i} completed successfully!\")\n",
    "            print(f\"   - Patients evaluated: {len(patient_df)}\")\n",
    "            print(f\"   - Sites evaluated: {len(site_df)}\")\n",
    "            print(f\"   - Files saved: {len(saved_files)}\")\n",
    "            \n",
    "            # Print key metrics if available\n",
    "            if metrics:\n",
    "                for task_name in ['TB Label', 'Pneumonia Label', 'Covid Label']:\n",
    "                    task_metrics = {k.replace(f'{task_name}_', ''): v for k, v in metrics.items() \n",
    "                                  if k.startswith(f'{task_name}_')}\n",
    "                    if task_metrics:\n",
    "                        auc = task_metrics.get('auc', 'N/A')\n",
    "                        acc = task_metrics.get('accuracy', 'N/A')\n",
    "                        print(f\"   - {task_name}: AUC={auc:.4f if isinstance(auc, (int, float)) else auc}, ACC={acc:.4f if isinstance(acc, (int, float)) else acc}\")\n",
    "            \n",
    "            successful_folds.append(i)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing fold {i}: {e}\")\n",
    "            failed_folds.append(i)\n",
    "            \n",
    "            # Print traceback for debugging\n",
    "            import traceback\n",
    "            print(\"Full error traceback:\")\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # Continue with next fold\n",
    "            continue\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ“Š EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"âœ… Successful folds: {successful_folds} ({len(successful_folds)}/{len(valid_folds)})\")\n",
    "    if failed_folds:\n",
    "        print(f\"âŒ Failed folds: {failed_folds}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ Results saved to: /gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/results_Aug11v1\")\n",
    "    \n",
    "    if successful_folds:\n",
    "        print(f\"\\nðŸŽ‰ Evaluation completed for {len(successful_folds)} folds!\")\n",
    "        print(\"Check the results directory for detailed outputs:\")\n",
    "        print(\"  - CSV files: patient and site-level predictions\")\n",
    "        print(\"  - HDF5 files: complex model outputs and features\")\n",
    "        print(\"  - JSON files: evaluation metrics\")\n",
    "    else:\n",
    "        print(\"âŒ No folds completed successfully. Please check the errors above.\")\n",
    "\n",
    "    # Optional: Create a consolidated summary across all folds\n",
    "    if len(successful_folds) > 1:\n",
    "        print(f\"\\nðŸ“ˆ Creating consolidated summary across {len(successful_folds)} folds...\")\n",
    "        try:\n",
    "            create_cross_fold_summary(successful_folds, '/gpfs/gibbs/project/hartley/tjb76/artstuff_OPTIMIZEDWOOOO/ULTR-CLIP/results_Aug26')\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Warning: Could not create cross-fold summary: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
